{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlwM14dkEpsjD9j49bZYtP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cbd1p8Ypzd-L","executionInfo":{"status":"ok","timestamp":1720419443009,"user_tz":-540,"elapsed":2966,"user":{"displayName":"박창현","userId":"13387938842588216228"}},"outputId":"08aa6a5b-d704-4af2-abdb-ce1faa885690"},"outputs":[{"output_type":"stream","name":"stdout","text":["부분 데이터셋을 사용한 모델의 정확도: 0.70\n","전체 데이터셋을 사용한 모델의 정확도: 1.00\n"]}],"source":["# 리스트 중첩 슬라이싱이 머신러닝에서 사용예시 :\n","\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# 데이터 로드\n","iris = load_iris()\n","x, y = iris.data, iris.target\n","\n","# 데이터 분할\n","x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=42)\n","\n","\n","# 중첩된 슬라이싱을 사용하여 부분 데이터셋 생성\n","# 훈련 데이터의 첫 50개 샘플에서 짝수 인덱스의 첫 두 특성만 선택\n","x_train_subset = x_train[:50:2, :2]\n","y_train_subset = y_train[:50:2]\n","\n","# 테스트 데이터의 마지막 20개 샘플에서 홀수 인덱스의 마지막 두 특성만 선택\n","x_test_subset = x_test[-20::2, 2:]\n","y_test_subset = y_test[-20::2]\n","\n","# 모델 생성 및 학습\n","model = SVC(kernel = 'linear', random_state = 42)\n","model.fit(x_train_subset, y_train_subset)\n","\n","# 예측\n","y_pred = model.predict(x_test_subset)\n","\n","# 정확도 계산\n","accuracy = accuracy_score(y_test_subset, y_pred)\n","print(f\"부분 데이터셋을 사용한 모델의 정확도: {accuracy:.2f}\")\n","\n","# 전체 데이터셋으로 모델 재학습 및 평가\n","full_model = SVC(kernel='linear', random_state = 42)\n","full_model.fit(x_train, y_train)\n","full_accuracy= accuracy_score(y_test, full_model.predict(x_test))\n","print(f\"전체 데이터셋을 사용한 모델의 정확도: {full_accuracy:.2f}\")"]},{"cell_type":"markdown","source":["중첩된 슬라이싱과 리스트 컴프리헨션은 둘 다 파이썬에서 데이터를 조작하고 부분 집합을 생성하는 데 사용되는 강력한 도구입니다. 각각의 특징과 차이점을 비교하여 설명해 드리겠습니다.\n","\n"," - 중첩된 슬라이싱:\n","    - 문법 : 리스트[시작:끝:단계][시작:끝:단계]\n","    - 다차원 배열이나 리스트에서 부분 집합을 추출하는 데 사용됩니다.\n","    - 인덱싱 기반으로 작동하며, 원본 데이터 구조의 형태를 유지합니다.\n","    - 주로 numpy 배열이나 다차원 리스트에서 사용됩니다.\n","\n"," - 리스트 컴프리헨션:\n","    - 문법 : [표현식 for 항목 in 반복가능객체 if 조건]\n","    - 기존 리스트를 기반으로 새로운 리스트를 생성하는 데 사용됩니다.\n","    - 각 요소에 대해 조건을 확인하고 변환을 적용할 수 있습니다.\n","    - 1차원 리스트 생성에 주로 사용되지만, 중첩하여 다차원 리스트도 만들 수 있습니다."],"metadata":{"id":"c0S9xyEP2zXn"}},{"cell_type":"code","source":["# 리스트 컴프리헨션이 사용된 머신러닝 사례 코드\n","\n","# 리스트 컴프리헨션의 사용 사례:\n","\n","texts = [review for review, _ in data] # 데이터에서 텍스트만 추출\n","labels = [label for _, label in data] # 데이터에서 레이블만 추출\n","preprocessed_texts = [text.lower() for text in texts]  # 텍스트를 소문자로 변환\n","\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# 샘플 데이터 : (텍스트, 레이블) 형태의 튜플 리스트\n","\n","data = [\n","    (\"I love this movie\", \"positive\"),\n","    (\"Great acting and plot\", \"positive\"),\n","    (\"Terrible waste of time\", \"negative\"),\n","    (\"Boring and predictable\", \"negative\"),\n","    (\"Amazing special effects\", \"positive\"),\n","    (\"Worst movie ever\", \"negative\"),\n","    (\"Highly recommended\", \"positive\"),\n","    (\"Don't bother watching\", \"negative\")\n","]\n","\n","# 리스트 컴프리헨션을 사용한 간단한 전철\n","preprocessed_texts = [text.lower() for text in texts]\n","\n","# 데이터 학습 세트와 테스트 세트로 분할\n","x_train, x_test, y_train, y_test = train_test_split(preprocessed_texts, labels, test_size=0.2, random_state=42)\n","\n","# CountVectorizer를 사용하여 텍스트를 특성 벡터로 변환\n","vectorizer = CountVectorizer()\n","x_train_vectorized = vectorizer.fit_transform(x_train)\n","x_test_vectorized = vectorizer.transform(x_test)\n","\n","# 나이브 베이즈 분류기 학습\n","classifier = MultinomialNB()\n","classifier.fit(x_train_vectorized, y_train)\n","\n","# 예측\n","y_pred = classifier.predict(x_test_vectorized)\n","\n","# 정확도 평가\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"정확도: {accuracy:.2f}\")\n","\n","# 분류 보고서\n","print(\"\\n분류 보고서: \")\n","print(classification_report(y_test, y_pred))\n","\n","# 리스트 컴프리헨션을 사용하여 새로운 리뷰 예측\n","new_reviews = [\n","    \"This movie was fantastic\",\n","    \"I didn't enjoy it at all\",\n","    \"The actors were great but the story was weak\"\n","]\n","\n","new_reviews_vectorized = vectorizer.transform(new_reviews)\n","predictions = classifier.predict(new_reviews_vectorized)\n","\n","print(\"\\n새로운 리뷰 예측: \")\n","[print(f\"리뷰: '{review}'\\n예측: {prediction}\\n\") for review, prediction in zip(new_reviews, predictions)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqmGofnM0FkA","executionInfo":{"status":"ok","timestamp":1720401210455,"user_tz":-540,"elapsed":398,"user":{"displayName":"박창현","userId":"13387938842588216228"}},"outputId":"645de477-c1c4-4f14-f90d-3781ad7c2e09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["정확도: 0.00\n","\n","분류 보고서: \n","              precision    recall  f1-score   support\n","\n","    negative       0.00      0.00      0.00       1.0\n","    positive       0.00      0.00      0.00       1.0\n","\n","    accuracy                           0.00       2.0\n","   macro avg       0.00      0.00      0.00       2.0\n","weighted avg       0.00      0.00      0.00       2.0\n","\n","\n","새로운 리뷰 예측: \n","리뷰: 'This movie was fantastic'\n","예측: positive\n","\n","리뷰: 'I didn't enjoy it at all'\n","예측: negative\n","\n","리뷰: 'The actors were great but the story was weak'\n","예측: negative\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[None, None, None]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# List comprehension을 사용한 머신 러닝 예시코드\n","# 아이리스 데이터셋을 사용하여 간단한 KNN(K-Nearest Neighbors)분류기를 구현합니다.\n","# 리스트 컴프리헨션을 여러 곳에서 활용하고 있습니다. 주요 특징은 다음과 같습니다.\n","\n","# 데이터 전처리:\n","# x_selected = [[feature[0], feature[2]] for feature in x]\n","#   : 리스트 컴프리헨션을 사용하여 꽃받침 길이와 꽃잎 길이만 선택합니다.\n","# y_transformed = [y_names[label] for label in y] : 숫자 레이블을 꽃 이름으로 변환합니다.\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 아이리스 데이터셋 로드\n","iris = load_iris()\n","x, y = iris.data, iris.target\n","\n","# 리스트 컴프리헨션을 사용한 데이터 전처리\n","# 꽃받침 길이와 꽃잎 길이만 선택(인덱스 0과 2)\n","x_selected = [[feature[0], feature[2]] for feature in x]\n","\n","# 리스트 컴프리헨션을 사용한 레이블 변환\n","# 0 -> 'setosa', 1-> 'versicolor', 2 -> 'viginica'\n","y_names = ['setosa','versicolor','virginica']\n","y_transformed = [y_names[label] for label in y]\n","\n","# 데이터 분할\n","x_train, x_test, y_train, y_test = train_test_split(x_selected, y_transformed, test_size=0.3, random_state=42)\n","\n","# KNN모델 훈련\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(x_train, y_train)\n","\n","# 예측\n","y_pred = knn.predict(x_test)\n","\n","# 정확도 계산\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"모델 정확도: {accuracy:.2f}\")\n","\n","# 리스트 컴프리헨션을 사용한 결과 출력\n","print(\"\\n예측 결과: \")\n","[print(f\"실제: {true}, 예측: {pred}\") for true, pred in zip(y_test, y_pred)]\n","\n","# 새로운 데이터에 대한 예측\n","new_flowers = [[5.1, 1.8], [4.9, 1.5], [6.3,5.0]]\n","new_predictions = knn.predict(new_flowers)\n","\n","print(\"\\n새로운 꽃 예측: \")\n","[print(f\"꽃받침 길이: {flower[0]}, 꽃잎 길이: {flower[1]}  -> 예측: {prediction}\")\n","for flower, prediction in zip(new_flowers,new_predictions)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VB_kq_Co5KEP","executionInfo":{"status":"ok","timestamp":1720403738735,"user_tz":-540,"elapsed":591,"user":{"displayName":"박창현","userId":"13387938842588216228"}},"outputId":"a6d5c1ad-36f2-416c-9911-f65eb403aaaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["모델 정확도: 0.96\n","\n","예측 결과: \n","실제: versicolor, 예측: versicolor\n","실제: setosa, 예측: setosa\n","실제: virginica, 예측: virginica\n","실제: versicolor, 예측: versicolor\n","실제: versicolor, 예측: versicolor\n","실제: setosa, 예측: setosa\n","실제: versicolor, 예측: versicolor\n","실제: virginica, 예측: versicolor\n","실제: versicolor, 예측: versicolor\n","실제: versicolor, 예측: versicolor\n","실제: virginica, 예측: virginica\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: versicolor, 예측: virginica\n","실제: virginica, 예측: virginica\n","실제: versicolor, 예측: versicolor\n","실제: versicolor, 예측: versicolor\n","실제: virginica, 예측: virginica\n","실제: setosa, 예측: setosa\n","실제: virginica, 예측: virginica\n","실제: setosa, 예측: setosa\n","실제: virginica, 예측: virginica\n","실제: virginica, 예측: virginica\n","실제: virginica, 예측: virginica\n","실제: virginica, 예측: virginica\n","실제: virginica, 예측: virginica\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: versicolor, 예측: versicolor\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: virginica, 예측: virginica\n","실제: versicolor, 예측: versicolor\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","실제: virginica, 예측: virginica\n","실제: versicolor, 예측: versicolor\n","실제: versicolor, 예측: versicolor\n","실제: setosa, 예측: setosa\n","실제: setosa, 예측: setosa\n","\n","새로운 꽃 예측: \n","꽃받침 길이: 5.1, 꽃잎 길이: 1.8  -> 예측: setosa\n","꽃받침 길이: 4.9, 꽃잎 길이: 1.5  -> 예측: setosa\n","꽃받침 길이: 6.3, 꽃잎 길이: 5.0  -> 예측: virginica\n"]},{"output_type":"execute_result","data":{"text/plain":["[None, None, None]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# 딕셔너리의 머신러닝 사용 예시코드\n","\n","# 하이퍼파라미터를 딕셔너리로 관리\n","\n","hyperparameters = {\n","    'learning_rate' : 0.001,\n","    'batch_size' : 32,\n","    'epochs' : 10,\n","    'optimizer' : 'adam'\n","}\n","\n","# 예시로 Kearas를 사용하는 경우\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Iris 데이터셋 호출\n","iris = load_iris()\n","x = iris.data\n","y = iris.target.reshape(-1,1) # (150,) -> (150,1)\n","\n","# OneHot 인코딩\n","encoder = OneHotEncoder(sparse_output=False)\n","y = encoder.fit_transform(y) # (150,1) -> (150,3)\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","\n","model = Sequential()\n","model.add(Dense(64, activation='relu', input_shape=(x_train.shape[1],)))\n","model.add(Dense(3, activation='softmax')) # 3개의 클래스를 위한 출력층\n","\n","optimizer = Adam(learning_rate = hyperparameters['learning_rate'])\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics = ['accuracy'])\n","\n","# 모델학습\n","model.fit(x_train, y_train, batch_size = hyperparameters['batch_size'], epochs = hyperparameters['epochs'])\n","\n","# 모델 평가\n","score = model.evaluate(x_test, y_test)\n","print(f'\\n\\nTest loss: {score[0]} / Test accuracy: {score[1]}')\n","\n","# 예측\n","predictions = model.predict(x_test)\n","\n","# 혼동 행렬 출력\n","y_pred_classes = np.argmax(predictions, axis=1)\n","y_test_classes = np.argmax(y_test, axis=1)\n","conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n","print(\"\\n\\nConfusion Matrix: \")\n","print(conf_matrix)\n","\n","# 분류 보고서 출력\n","class_names = iris.target_names\n","print(\"\\nClassification Report: \")\n","print(classification_report(y_test_classes, y_pred_classes, target_names = class_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VeorXwTnDZwO","executionInfo":{"status":"ok","timestamp":1720421009960,"user_tz":-540,"elapsed":2871,"user":{"displayName":"박창현","userId":"13387938842588216228"}},"outputId":"2fcdfa66-72b3-4de1-f8ca-e83e075e77dd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","4/4 [==============================] - 1s 6ms/step - loss: 1.2761 - accuracy: 0.3250\n","Epoch 2/10\n","4/4 [==============================] - 0s 9ms/step - loss: 1.1291 - accuracy: 0.3250\n","Epoch 3/10\n","4/4 [==============================] - 0s 6ms/step - loss: 1.0249 - accuracy: 0.3250\n","Epoch 4/10\n","4/4 [==============================] - 0s 6ms/step - loss: 0.9616 - accuracy: 0.3250\n","Epoch 5/10\n","4/4 [==============================] - 0s 7ms/step - loss: 0.9291 - accuracy: 0.4167\n","Epoch 6/10\n","4/4 [==============================] - 0s 8ms/step - loss: 0.9003 - accuracy: 0.5833\n","Epoch 7/10\n","4/4 [==============================] - 0s 6ms/step - loss: 0.8705 - accuracy: 0.6333\n","Epoch 8/10\n","4/4 [==============================] - 0s 11ms/step - loss: 0.8363 - accuracy: 0.6583\n","Epoch 9/10\n","4/4 [==============================] - 0s 6ms/step - loss: 0.8050 - accuracy: 0.7833\n","Epoch 10/10\n","4/4 [==============================] - 0s 6ms/step - loss: 0.7736 - accuracy: 0.7500\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7922f0ae88b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 335ms/step - loss: 0.7412 - accuracy: 0.7667\n","\n","\n","Test loss: 0.7411728501319885 / Test accuracy: 0.7666666507720947\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7922f0aea9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 236ms/step\n","\n","\n","Confusion Matrix: \n","[[10  0  0]\n"," [ 0  2  7]\n"," [ 0  0 11]]\n","\n","Classification Report: \n","              precision    recall  f1-score   support\n","\n","      setosa       1.00      1.00      1.00        10\n","  versicolor       1.00      0.22      0.36         9\n","   virginica       0.61      1.00      0.76        11\n","\n","    accuracy                           0.77        30\n","   macro avg       0.87      0.74      0.71        30\n","weighted avg       0.86      0.77      0.72        30\n","\n"]}]},{"cell_type":"code","source":["# 튜플의 머신러닝 사용예시 코드\n","import tensorflow as tf\n","\n","# 예제데이터\n","features = [[1,2],[3,4],[5,6]]\n","labels = [0,1,0]\n","\n","# 튜플을 사용하여 데이터셋 생성\n","dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n","\n","for element in dataset:\n","  print(element)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZHlxuR-HgfO","executionInfo":{"status":"ok","timestamp":1720405040609,"user_tz":-540,"elapsed":395,"user":{"displayName":"박창현","userId":"13387938842588216228"}},"outputId":"3b1bb33d-c227-407c-8a7a-7b1668b8dbc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n","(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n","(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([5, 6], dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"]}]},{"cell_type":"code","source":["# 정규표현식의 머신러닝 사용 예시코드\n","# 머신러닝 모델에 입력하기 전에 텍스트를 정제하는 과정을 보여줍니다.\n","# URL, HTML 태그, 이메일 주소, 특수 문자 등을 제거하고 텍스트를 일관된 형식으로 만듭니다.\n","\n","import re\n","\n","def clean_text(text):\n","  # 소문자 변환\n","  text = text.lower()\n","\n","  # URL 제거\n","  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","\n","  # HTML 태그 제거\n","  text = re.sub(r'<.*?>','',text)\n","\n","  # 이메일 주소 제거\n","  text = re.sub(r'\\S+@|S+','',text)\n","\n","  # 특수 문자 제거 (단, 일부 문장 부호는 유지)\n","  text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?]','',text)\n","\n","  # 연속된 공백 제거\n","  text = re.sub(r'\\s|','',text).strip()\n","\n","  return text\n","\n","# 테스트\n","sample_text = \"\"\"\n","Check out our website at http://www.example.com!\n","Contact us at <email>info@example.com</email>.\n","#MachineLearning is awesome! Don't you think so???\n","\"\"\"\n","\n","cleaned_text = clean_text(sample_text)\n","print(cleaned_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-8qHLeTH8Xu","executionInfo":{"status":"ok","timestamp":1720405321619,"user_tz":-540,"elapsed":518,"user":{"displayName":"박창현","userId":"13387938842588216228"}},"outputId":"0a354d7a-b76b-48c5-ec8a-11e0399c018a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["checkoutourwebsiteatcontactusatexample.com.machinelearningisawesome!dontyouthinkso???\n"]}]},{"cell_type":"code","source":["# Deep copy() 를 사용하는 사례의 코드 (원본 모델의 내부상태(가중치, 편향 등)을 복제)\n","\n","import copy\n","import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","\n","# 데이터 생성\n","x, y = make_classification(n_samples = 1000, n_features = 20, n_classes = 2, random_state = 42)\n","x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)\n","\n","# 기본 모델 생성\n","base_model = MLPClassifier(hidden_layer_sizes = (100,50), max_iter = 200, random_state=42)\n","base_model.fit(x_train, y_train)\n","\n","print(\"기본 모델 정확도: \", base_model.score(x_test,y_test))\n","\n","# 실험을 위한 모델 복사 및 수정\n","experiments = []\n","learning_rates = [0.001, 0.01, 0.1]\n","\n","for lr in learning_rates:\n","  # deep copy 를 사용하여 모델 복사\n","  experiment_model = copy.deepcopy(base_model)\n","\n","  # 학습률 수정\n","  experiment_model.learning_rate_init = lr\n","\n","  # 추가 학습\n","  experiment_model.partial_fit(x_train, y_train)\n","\n","  accuracy = experiment_model.score(x_test, y_test)\n","  experiments.append((lr, accuracy))\n","\n","# 실험 결과 출력\n","for lr, accuracy in experiments:\n","  print(f\"학습률 {lr}의 모델 정확도 : {accuracy}\")"],"metadata":{"id":"3t_9Lo8QIq_V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720419740983,"user_tz":-540,"elapsed":4577,"user":{"displayName":"박창현","userId":"13387938842588216228"}},"outputId":"00f52f36-74a1-405c-f27e-e24ad9bee843"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["기본 모델 정확도:  0.82\n","학습률 0.001의 모델 정확도 : 0.82\n","학습률 0.01의 모델 정확도 : 0.82\n","학습률 0.1의 모델 정확도 : 0.82\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Qc_suey9Adm_"},"execution_count":null,"outputs":[]}]}